{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkTTDnJVQTeU",
        "outputId": "315c5a9c-511b-4b71-9032-c32656895af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.1964153406512031\n",
            "Epoch 2/20, Loss: 0.18696593103820022\n",
            "Epoch 3/20, Loss: 0.18565114330539303\n",
            "Epoch 4/20, Loss: 0.18280902193398427\n",
            "Epoch 5/20, Loss: 0.18237417653358773\n",
            "Epoch 6/20, Loss: 0.18123335666461274\n",
            "Epoch 7/20, Loss: 0.18029561252024948\n",
            "Epoch 8/20, Loss: 0.17826775783341936\n",
            "Epoch 9/20, Loss: 0.17916872087567526\n",
            "Epoch 10/20, Loss: 0.17774937875594826\n",
            "Epoch 11/20, Loss: 0.17683998822439007\n",
            "Epoch 12/20, Loss: 0.1752000514690469\n",
            "Epoch 13/20, Loss: 0.17467091066392873\n",
            "Epoch 14/20, Loss: 0.17359068127874713\n",
            "Epoch 15/20, Loss: 0.17311766678759447\n",
            "Epoch 16/20, Loss: 0.17165864329097164\n",
            "Epoch 17/20, Loss: 0.17146992358908006\n",
            "Epoch 18/20, Loss: 0.17035381612773556\n",
            "Epoch 19/20, Loss: 0.16950339115247492\n",
            "Epoch 20/20, Loss: 0.16878650446490542\n",
            "Test Loss: 0.22497546155419615\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('prepared_dataset.csv')\n",
        "\n",
        "# Step 2: Preprocess the data (for simplicity, assuming the dataset has a numeric feature column)\n",
        "# Example: We'll normalize all columns except the 'target' column.\n",
        "\n",
        "# Normalize the features\n",
        "scaler = MinMaxScaler()\n",
        "features = df.drop(columns=['will_fail']).values\n",
        "target = df['will_fail'].values\n",
        "\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Step 3: Prepare the data for LSTM (creating sequences for time series problems)\n",
        "def create_sequences(data, target, seq_length=30):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data[i:i+seq_length])\n",
        "        labels.append(target[i+seq_length])\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(scaled_features, target)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Step 5: Create DataLoader for batching\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Step 6: Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # We use the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Step 7: Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_layer_size = 64\n",
        "output_size = 1  # Assuming regression or single target prediction\n",
        "model = LSTMModel(input_size, hidden_layer_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()  # Use cross entropy loss for classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 8: Train the model\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(batch_X)\n",
        "        loss = criterion(y_pred.squeeze(), batch_y)  # Squeeze to match shapes\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}\")\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        y_pred = model(batch_X)\n",
        "        loss = criterion(y_pred.squeeze(), batch_y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
        "\n",
        "# You can also implement additional evaluation metrics like R^2, accuracy, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        logits = model(batch_X)\n",
        "        predicted = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(all_labels, all_preds)\n",
        "print(\"\\nðŸ“‹ Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_E4ELMOVagn",
        "outputId": "e3665621-6f8c-45ea-b7ee-4b75ecf9884d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“‹ Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      1.00      0.83      3251\n",
            "         1.0       0.00      0.00      0.00      1333\n",
            "\n",
            "    accuracy                           0.71      4584\n",
            "   macro avg       0.35      0.50      0.41      4584\n",
            "weighted avg       0.50      0.71      0.59      4584\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}