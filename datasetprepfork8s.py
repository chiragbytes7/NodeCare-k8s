# -*- coding: utf-8 -*-
"""DatasetPrepfork8s.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HbpB5LonfYh9IXO_3Nct0VaQICoz0QjS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
df = pd.read_csv('dataSynthetic.csv')

# Convert timestamp to datetime
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Set timestamp as index for time series analysis
df_indexed = df.set_index('Timestamp')

# Get basic statistics and understanding
print(df.describe())
print(df['Pod Status'].value_counts())
print(df['Pod Reason'].value_counts())

# Create a function to identify failure events
def identify_failures(df):
    # Group by pod name to track changes over time
    pod_groups = df.sort_values('Timestamp').groupby('Pod Name')

    # Initialize empty dataframe for failures
    failures = pd.DataFrame(columns=['Pod Name', 'Failure Time', 'Failure Type'])

    for pod_name, pod_data in pod_groups:
        # 1. Status transitions to Failed/Error
        status_failures = pod_data[
            (pod_data['Pod Status'].isin(['Failed', 'Error', 'CrashLoopBackOff'])) &
            (pod_data['Pod Status'].shift(1) != pod_data['Pod Status'])
        ]

        # 2. Pod restarts
        restart_changes = pod_data['Pod Restarts'].diff() > 0
        restart_failures = pod_data[restart_changes]

        # 3. Ready containers < Total containers for prolonged period
        container_issues = pod_data[
            (pod_data['Ready Containers'] < pod_data['Total Containers']) &
            # Check if condition persists for multiple observations (adjust as needed)
            (pod_data['Ready Containers'].shift(1) < pod_data['Total Containers'].shift(1)) &
            (pod_data['Ready Containers'].shift(2) < pod_data['Total Containers'].shift(2))
        ]


        # 5. High error event counts
        error_events = pod_data[
            pod_data['Pod Event Type'] == 'Warning'
        ]

        # Combine all identified failures
        all_failures = pd.concat([
            status_failures.assign(Failure_Type='Status Change'),
            restart_failures.assign(Failure_Type='Restart'),
            container_issues.assign(Failure_Type='Container Issue'),
            error_events.assign(Failure_Type='Warning Event')
        ])

        # Add to overall failures dataframe
        if not all_failures.empty:
            for idx, row in all_failures.iterrows():
                failures = pd.concat([failures, pd.DataFrame({
                    'Pod Name': [pod_name],
                    'Failure Time': [row.name],  # Using the timestamp index
                    'Failure Type': [row['Failure_Type']]
                })])

    return failures.drop_duplicates()

# Generate failure events
failure_events = identify_failures(df_indexed)


"""addin a failure column, rolling avgs and removing the rows after failure for a particular pod

"""

print(failure_events.head(1))

# Function to create target variable with prediction window
def create_target_labels(df, failures, prediction_window='30min'):
    # Create a copy of the original dataframe
    labeled_df = df.copy()

    # Initialize the target column
    labeled_df['will_fail'] = 0

    # For each failure, mark the preceding period as leading to failure
    for idx, failure in failures.iterrows():
        pod_name = failure['Pod Name']
        failure_time = failure['Failure Time']

        # Find records for this pod within the prediction window before failure
        window_start = failure_time - pd.Timedelta(prediction_window)

        # Mark records in the prediction window as "will fail"
        mask = (labeled_df['Pod Name'] == pod_name) & \
               (labeled_df.index > window_start) & \
               (labeled_df.index < failure_time)

        labeled_df.loc[mask, 'will_fail'] = 1

    # IMPORTANT: Remove all records at or after failure events to prevent leakage
    for idx, failure in failures.iterrows():
        pod_name = failure['Pod Name']
        failure_time = failure['Failure Time']

        # Remove data at the failure time and shortly after (recovery period)
        recovery_period = failure_time + pd.Timedelta('10min')
        mask = (labeled_df['Pod Name'] == pod_name) & \
               (labeled_df.index >= failure_time) & \
               (labeled_df.index <= recovery_period)

        labeled_df = labeled_df.drop(labeled_df[mask].index)
        labeled_df = labeled_df.sort_index().reset_index()
        labeled_df = labeled_df.set_index('Timestamp')
    return labeled_df

# Create labeled dataset
labeled_df = create_target_labels(df_indexed, failure_events, prediction_window='30min')


def engineer_features(df):
    # Ensure Timestamp is in datetime format

    # Create a copy to avoid modifying the original
    df_features = df.copy()

    # Define feature calculations using groupby().transform()
    df_features['cpu_avg_5min'] = df_features.groupby('Pod Name')['CPU Usage (%)'].transform(
        lambda x: x.rolling('5min', closed='left').mean()
    )
    df_features['cpu_avg_15min'] = df_features.groupby('Pod Name')['CPU Usage (%)'].transform(
        lambda x: x.rolling('15min', closed='left').mean()
    )
    df_features['memory_avg_5min'] = df_features.groupby('Pod Name')['Memory Usage (%)'].transform(
        lambda x: x.rolling('5min', closed='left').mean()
    )
    df_features['memory_avg_15min'] = df_features.groupby('Pod Name')['Memory Usage (%)'].transform(
        lambda x: x.rolling('15min', closed='left').mean()
    )

    # Compute network traffic as sum of received and transmitted packets
    df_features['net_traffic'] = df_features['Network Receive Packets (p/s)'].fillna(0) + df_features['Network Transmit Packets (p/s)'].fillna(0) # Assign the Series to a new column 'net_traffic' within the DataFrame
    df_features['net_traffic_avg_5min'] = df_features.groupby('Pod Name')['net_traffic'].transform(
        lambda x: x.rolling('5min', closed='left').mean()
    )

    # Compute network errors
    df_features['net_errors'] = df_features['Network Receive Packets Dropped (p/s)'].fillna(0) + df_features['Network Transmit Packets Dropped (p/s)'].fillna(0)
    df_features['net_errors_5min'] = df_features.groupby('Pod Name')['net_errors'].transform(
        lambda x: x.rolling('5min', closed='left').sum()
    )

    # Calculate CPU and memory trends using polyfit slope (for 10-minute windows)
    df_features['cpu_trend'] = df_features.groupby('Pod Name')['CPU Usage (%)'].transform(
        lambda x: x.rolling('10min', closed='left').apply(
            lambda y: np.nan if len(y) < 3 else np.polyfit(range(len(y)), y, 1)[0] if np.isfinite(y).all() else np.nan
        )
    )
    df_features['memory_trend'] = df_features.groupby('Pod Name')['Memory Usage (%)'].transform(
        lambda x: x.rolling('10min', closed='left').apply(
            lambda y: np.nan if len(y) < 3 else np.polyfit(range(len(y)), y, 1)[0] if np.isfinite(y).all() else np.nan
        )
    )

    # Count restarts in the last 6 hours
    restart_diff = df_features.groupby('Pod Name')['Pod Restarts'].diff().clip(lower=0)
    df_features['restart_count_6hr'] = df_features.groupby('Pod Name')[restart_diff.name].transform(
        lambda x: x.rolling('6h', closed='left').sum()
    )

    # Compute time since last restart (in hours)
    def time_since_restart(series):
        last_restart = None
        times = []
        for idx, value in series.items():
            if value > 0:
                last_restart = idx
            times.append((idx - last_restart).total_seconds() / 3600 if last_restart else 0)
        return pd.Series(times, index=series.index)

    df_features['time_since_last_restart'] = df_features.groupby('Pod Name')['Pod Restarts'].transform(time_since_restart)

    # Count error events in the last hour
    df_features['error_event_count_1hr'] = df_features.groupby('Pod Name')['Pod Event Type'].transform(
        lambda x: x.eq('Warning').rolling('1h', closed='left').sum()
    )

    # Compute warning event ratio over the last 6 hours
    df_features['warning_event_ratio'] = df_features.groupby('Pod Name')['Pod Event Type'].transform(
        lambda x: x.eq('Warning').rolling('6h', closed='left').sum() / x.rolling('6h', closed='left').count()
    ).fillna(0)

    return df_features

feature_df = engineer_features(labeled_df)

print(feature_df.head(1))

print("Number of Columns:", len(feature_df.columns))

def prepare_final_dataset(df):
    # Select relevant columns (removing leaky indicators)
    leaky_columns = [
        'Pod Status', 'Pod Reason', 'Error Message', 'Pod Event Type',
        'Pod Event Reason', 'Pod Event Message', 'Event Reason', 'Event Message',
        'Last Log Entry'  # Raw logs are leaky
    ]

    # Keep engineered features and non-leaky original features
    features_to_keep = [col for col in df.columns if col not in leaky_columns]
    final_df = df[features_to_keep]

    # Fill missing values appropriately
    numeric_cols = final_df.select_dtypes(include=['float64', 'int64']).columns
    final_df[numeric_cols] = final_df[numeric_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)

    # Handle high-cardinality categorical features

    # 1. For Pod Name: Instead of one-hot encoding, create pod-level statistics
    pod_stats = df.groupby('Pod Name')['Pod Restarts'].agg(['mean', 'max']).reset_index()
    pod_stats.columns = ['Pod Name', 'pod_avg_restarts', 'pod_max_restarts']

    # Add failure rate by pod
    pod_failure_rate = df.groupby('Pod Name')['will_fail'].mean().reset_index()
    pod_failure_rate.columns = ['Pod Name', 'pod_historical_failure_rate']

    # Merge these statistics back instead of one-hot encoding
    final_df = final_df.merge(pod_stats, on='Pod Name', how='left')
    final_df = final_df.merge(pod_failure_rate, on='Pod Name', how='left')

    # 2. For Node Name: Create node-level features instead of one-hot encoding
    if 'Node Name' in final_df.columns:
        node_stats = df.groupby('Node Name')['will_fail'].mean().reset_index()
        node_stats.columns = ['Node Name', 'node_historical_failure_rate']
        final_df = final_df.merge(node_stats, on='Node Name', how='left')

    # 3. For other categorical variables, use frequency encoding or limit categories
    cat_cols = final_df.select_dtypes(include=['object']).columns
    cat_cols = [col for col in cat_cols if col not in ['Pod Name', 'Node Name']]

    for col in cat_cols:
        # Count frequency of each category
        value_counts = final_df[col].value_counts(normalize=True)

        # Keep only the top N most frequent values, group others as 'Other'
        top_n = 10  # Adjust based on your needs
        top_values = value_counts.nlargest(top_n).index

        # Replace rare categories with 'Other'
        final_df[col] = final_df[col].apply(lambda x: x if x in top_values else 'Other')

        # Now one-hot encode the reduced categories
        dummies = pd.get_dummies(final_df[col], prefix=col)
        final_df = pd.concat([final_df, dummies], axis=1)
        final_df.drop(col, axis=1, inplace=True)

    # Remove Pod Name and Node Name after extracting their statistics
    final_df.drop(['Pod Name', 'Node Name'], axis=1, inplace=True)

    # Drop rows with any remaining NaN values
    final_df = final_df.dropna()

    return final_df

final_dataset = prepare_final_dataset(feature_df)

print("Number of Columns:", len(final_dataset.columns))

print(final_dataset.keys())

from sklearn.model_selection import train_test_split

start_time = final_dataset.index.min()
end_time = final_dataset.index.max()

train_cutoff = start_time + 0.8*(end_time - start_time)

# Train-test split
train_df = final_dataset[final_dataset.index <= train_cutoff]
test_df = final_dataset[final_dataset.index > train_cutoff]

# Separate features and target
X_train = train_df.drop(['will_fail'], axis=1)
y_train = train_df['will_fail']
X_test = test_df.drop(['will_fail'], axis=1)
y_test = test_df['will_fail']

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")

final_dataset['will_fail'].value_counts()

final_dataset.to_csv('prepared_dataset.csv', index = True)

!ls

from google.colab import files
files.download('prepared_dataset.csv')

import xgboost as xgb
from sklearn.metrics import classification_report
import numpy as np

# Compute class weights manually (since XGBoost doesn't support class_weight directly)
class_counts = np.bincount(y_train)
total_samples = len(y_train)
num_classes = len(class_counts)

scale_pos_weight = class_counts[0] / class_counts[1]  # Ratio of majority to minority class

# Define XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',  # Binary classification
    scale_pos_weight=scale_pos_weight,  # Balancing for class imbalance
    eval_metric='logloss',  # Logarithmic loss function
    n_estimators=200,  # Number of boosting rounds
    learning_rate=0.05,  # Step size shrinkage
    max_depth=5,  # Depth of trees
    subsample=0.8,  # Subsample ratio for boosting
    colsample_bytree=0.8,  # Fraction of features per tree
    random_state=42
)

# Train XGBoost model
xgb_model.fit(X_train, y_train)

training = xgb_model.predict(X_train)
print(classification_report(y_train, training))

# Predict on test data
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate model
print(classification_report(y_test, y_pred_xgb))

len(df)

import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report, recall_score
from sklearn.model_selection import ParameterGrid

# Optimized parameter grid
param_grid = {
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.05],
    'gamma': [2.0, 3.0, 4.0],  # Higher gamma focuses more on hard examples
    'alpha': [0.3, 0.4, 0.5],  # Higher alpha gives more weight to minority class
    'scale_pos_weight': [3, 4, 5]  # Explicit class weighting
}

best_recall = 0
best_params = {}
best_model = None

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

for params in ParameterGrid(param_grid):
    current_params = {
        'max_depth': params['max_depth'],
        'learning_rate': params['learning_rate'],
        'scale_pos_weight': params['scale_pos_weight'],
        'eval_metric': 'logloss'
    }

    model = xgb.train(
        current_params,
        dtrain,
        num_boost_round=300,  # Increased rounds
        obj=focal_loss(alpha=params['alpha'], gamma=params['gamma']),
        early_stopping_rounds=50,
        evals=[(dtest, 'eval')],
        verbose_eval=False
    )

    y_pred = (model.predict(dtest) > 0.3).astype(int)  # Lower threshold
    recall = recall_score(y_test, y_pred, pos_label=1)

    if recall > best_recall:
        best_recall = recall
        best_params = params
        best_model = model
        print(f"New best recall: {best_recall:.2f} with params: {best_params}")

# Evaluate best model
print("\n=== FINAL BEST MODEL ===")
print(f"Best Recall for Class 1: {best_recall:.2f}")
print(f"Best Parameters: {best_params}")

# Try different thresholds
thresholds = [0.2, 0.3, 0.4]
for thresh in thresholds:
    y_pred = (best_model.predict(dtest) > thresh).astype(int)
    print(f"\nThreshold: {thresh:.1f}")
    print(classification_report(y_test, y_pred))

"""BEST MODEL AS OF NOW BELOW

"""

import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report

def train_xgboost_model(X_train, y_train, X_test, y_test):
    best_params = {
        'max_depth': 5,
        'learning_rate': 0.01,
        'scale_pos_weight': 3,
        'eval_metric': 'logloss'
    }

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)

    model = xgb.train(
        best_params,
        dtrain,
        num_boost_round=300,
        early_stopping_rounds=50,
        evals=[(dtest, 'eval')],
        verbose_eval=False
    )

    return model, dtest

def evaluate_model(model, dtest, y_test):
    best_threshold = 0.2
    y_pred = (model.predict(dtest) > best_threshold).astype(int)
    print("\n=== MODEL EVALUATION ===")
    print(f"Best Threshold: {best_threshold}")
    print(classification_report(y_test, y_pred))

# Example usage:
# model, dtest = train_xgboost_model(X_train, y_train, X_test, y_test)
# evaluate_model(model, dtest, y_test)